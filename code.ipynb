{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Znerf/diabetes-detection/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqoTj2_vbCFp",
        "outputId": "cc974588-d6be-481a-ae33-9548cdf97d86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio pandas"
      ],
      "metadata": {
        "id": "F5idfJpTXX2Q",
        "outputId": "4c845601-9808-4fc0-8373-9710ca130e01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost==1.7.5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oN9jcZSd-ul",
        "outputId": "ad3bc93c-7a5c-4138-d0f7-a07f038e5a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost==1.7.5\n",
            "  Downloading xgboost-1.7.5-py3-none-manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost==1.7.5) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost==1.7.5) (1.13.1)\n",
            "Downloading xgboost-1.7.5-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.1.2\n",
            "    Uninstalling xgboost-2.1.2:\n",
            "      Successfully uninstalled xgboost-2.1.2\n",
            "Successfully installed xgboost-1.7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Q5tXrHoAdOQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Data/dataset_diabetes.csv')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "IPYpkDX1dQ6a",
        "outputId": "cb6ed184-f7ae-4a90-91f5-6f98bcce4acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Diabetes_binary  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
              "0              0.0     1.0       0.0        1.0  26.0     0.0     0.0   \n",
              "1              0.0     1.0       1.0        1.0  26.0     1.0     1.0   \n",
              "2              0.0     0.0       0.0        1.0  26.0     0.0     0.0   \n",
              "3              0.0     1.0       1.0        1.0  28.0     1.0     0.0   \n",
              "4              0.0     0.0       0.0        1.0  29.0     1.0     0.0   \n",
              "\n",
              "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
              "0                   0.0           1.0     0.0  ...            1.0   \n",
              "1                   0.0           0.0     1.0  ...            1.0   \n",
              "2                   0.0           1.0     1.0  ...            1.0   \n",
              "3                   0.0           1.0     1.0  ...            1.0   \n",
              "4                   0.0           1.0     1.0  ...            1.0   \n",
              "\n",
              "   NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  Education  \\\n",
              "0          0.0      3.0       5.0      30.0       0.0  1.0   4.0        6.0   \n",
              "1          0.0      3.0       0.0       0.0       0.0  1.0  12.0        6.0   \n",
              "2          0.0      1.0       0.0      10.0       0.0  1.0  13.0        6.0   \n",
              "3          0.0      3.0       0.0       3.0       0.0  1.0  11.0        6.0   \n",
              "4          0.0      2.0       0.0       0.0       0.0  0.0   8.0        5.0   \n",
              "\n",
              "   Income  \n",
              "0     8.0  \n",
              "1     8.0  \n",
              "2     8.0  \n",
              "3     8.0  \n",
              "4     8.0  \n",
              "\n",
              "[5 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5af16d94-8e57-4143-957f-13ab3120662b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Diabetes_binary</th>\n",
              "      <th>HighBP</th>\n",
              "      <th>HighChol</th>\n",
              "      <th>CholCheck</th>\n",
              "      <th>BMI</th>\n",
              "      <th>Smoker</th>\n",
              "      <th>Stroke</th>\n",
              "      <th>HeartDiseaseorAttack</th>\n",
              "      <th>PhysActivity</th>\n",
              "      <th>Fruits</th>\n",
              "      <th>...</th>\n",
              "      <th>AnyHealthcare</th>\n",
              "      <th>NoDocbcCost</th>\n",
              "      <th>GenHlth</th>\n",
              "      <th>MentHlth</th>\n",
              "      <th>PhysHlth</th>\n",
              "      <th>DiffWalk</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>Education</th>\n",
              "      <th>Income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5af16d94-8e57-4143-957f-13ab3120662b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5af16d94-8e57-4143-957f-13ab3120662b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5af16d94-8e57-4143-957f-13ab3120662b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3e3dd422-dd0e-46f0-a35c-395e520bd847\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3e3dd422-dd0e-46f0-a35c-395e520bd847')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3e3dd422-dd0e-46f0-a35c-395e520bd847 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "7Jiav-u_ddFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y= df['Diabetes_binary']\n",
        "\n",
        "X=df.drop('Diabetes_binary',axis=1)\n",
        "# y.head()\n",
        "\n",
        "# X.head()\n"
      ],
      "metadata": {
        "id": "jZMMJAMIMyvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a StratifiedKFold object\n",
        "n_splits = 5  # Number of folds\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize a list to store the accuracy scores for each fold\n",
        "accuracy_scores = []"
      ],
      "metadata": {
        "id": "zKIt2P8PNU2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Iterate through the folds\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # Create DMatrix for XGBoost\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "    # Set parameters for the XGBoost model\n",
        "    params = {\n",
        "      'objective': 'binary:logistic',\n",
        "      'eval_metric': 'logloss',\n",
        "      'eta': 0.1,\n",
        "      'max_depth': 3\n",
        "    }\n",
        "\n",
        "    # Train the model\n",
        "    model = xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(dtest)\n",
        "\n",
        "    # Evaluate the predictions and store the accuracy score\n",
        "    accuracy = accuracy_score(y_test, y_pred.round())\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "    print(f\"Fold {fold + 1}: Accuracy = {accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWhG8woFMvxs",
        "outputId": "9b525eb1-a628-4c9e-98df-53f7567e74e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: Accuracy = 0.7524577410000707\n",
            "Fold 2: Accuracy = 0.7523162882806422\n",
            "Fold 3: Accuracy = 0.7518036497382939\n",
            "Fold 4: Accuracy = 0.7514499929268638\n",
            "Fold 5: Accuracy = 0.7523695006365823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average accuracy across all folds\n",
        "average_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
        "print(f\"Average Accuracy: {average_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7xLi9MSm85K",
        "outputId": "eba8ded8-ab4f-4617-8414-90c02854e821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.7501839065445155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# ... (data loading and preprocessing) ...\n",
        "\n",
        "# Create a StratifiedKFold object\n",
        "n_splits = 5  # Number of folds\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store accuracy scores for each fold and each model\n",
        "xgb_accuracy_scores = []\n",
        "rf_accuracy_scores = []\n",
        "\n",
        "# Iterate through the folds\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # XGBoost model training and evaluation\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "    params = {\n",
        "        # ... (your XGBoost parameters) ...\n",
        "    }\n",
        "    xgb_model = xgb.train(params, dtrain, num_boost_round=100)\n",
        "    xgb_y_pred = xgb_model.predict(dtest)\n",
        "    xgb_accuracy = accuracy_score(y_test, xgb_y_pred.round())\n",
        "    xgb_accuracy_scores.append(xgb_accuracy)\n",
        "\n",
        "    # Random Forest model training and evaluation\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  # Adjust parameters as needed\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_y_pred = rf_model.predict(X_test)\n",
        "    rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
        "    rf_accuracy_scores.append(rf_accuracy)\n",
        "\n",
        "    print(f\"Fold {fold + 1}: XGBoost Accuracy = {xgb_accuracy}, Random Forest Accuracy = {rf_accuracy}\")\n",
        "\n",
        "# Calculate average accuracy for each model\n",
        "average_xgb_accuracy = sum(xgb_accuracy_scores) / len(xgb_accuracy_scores)\n",
        "average_rf_accuracy = sum(rf_accuracy_scores) / len(rf_accuracy_scores)\n",
        "\n",
        "print(f\"Average XGBoost Accuracy: {average_xgb_accuracy}\")\n",
        "print(f\"Average Random Forest Accuracy: {average_rf_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsRnDTbkPLbu",
        "outputId": "7e4a0127-d6d0-4b28-a35c-f12d39a22154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: XGBoost Accuracy = 0.7463045477049296, Random Forest Accuracy = 0.7394440908126458\n",
            "Fold 2: XGBoost Accuracy = 0.7482141594172148, Random Forest Accuracy = 0.7376759318197892\n",
            "Fold 3: XGBoost Accuracy = 0.7502475597680012, Random Forest Accuracy = 0.7380110340925167\n",
            "Fold 4: XGBoost Accuracy = 0.7468524543782713, Random Forest Accuracy = 0.7307257037770547\n",
            "Fold 5: XGBoost Accuracy = 0.7498231715942849, Random Forest Accuracy = 0.7390720045268072\n",
            "Average XGBoost Accuracy: 0.7482883785725404\n",
            "Average Random Forest Accuracy: 0.7369857530057626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import lightgbm as lgb\n",
        "\n",
        "# ... (data loading and preprocessing) ...\n",
        "\n",
        "# Create a StratifiedKFold object\n",
        "n_splits = 5  # Number of folds\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store accuracy scores for each fold and each model\n",
        "xgb_accuracy_scores = []\n",
        "rf_accuracy_scores = []\n",
        "lgb_accuracy_scores = []\n",
        "\n",
        "# Iterate through the folds\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # XGBoost model training and evaluation\n",
        "    # ... (XGBoost code as before) ...\n",
        "\n",
        "    # Random Forest model training and evaluation\n",
        "    # ... (Random Forest code as before) ...\n",
        "\n",
        "    # LightGBM model training and evaluation\n",
        "    lgb_train = lgb.Dataset(X_train, label=y_train)\n",
        "    lgb_test = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
        "    params = {\n",
        "        'objective': 'binary',  # Adjust objective as needed\n",
        "        'metric': 'binary_logloss',  # Adjust metric as needed\n",
        "        # ... (other LightGBM parameters) ...\n",
        "    }\n",
        "    lgb_model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_test], num_boost_round=100)\n",
        "    lgb_y_pred = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n",
        "    lgb_accuracy = accuracy_score(y_test, (lgb_y_pred > 0.5).astype(int))  # Assuming binary classification\n",
        "    lgb_accuracy_scores.append(lgb_accuracy)\n",
        "\n",
        "    print(f\"Fold {fold + 1}: XGBoost Accuracy = {xgb_accuracy}, Random Forest Accuracy = {rf_accuracy}, LightGBM Accuracy = {lgb_accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEwgxLqvQHKr",
        "outputId": "8404754e-c622-46eb-f95c-6789ddc0d284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28276\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016168 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 56553, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500009 -> initscore=0.000035\n",
            "[LightGBM] [Info] Start training from score 0.000035\n",
            "Fold 1: XGBoost Accuracy = 0.7498231715942849, Random Forest Accuracy = 0.7390720045268072, LightGBM Accuracy = 0.7513261192446424\n",
            "[LightGBM] [Info] Number of positive: 28276, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011228 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 197\n",
            "[LightGBM] [Info] Number of data points in the train set: 56553, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000035\n",
            "[LightGBM] [Info] Start training from score -0.000035\n",
            "Fold 2: XGBoost Accuracy = 0.7498231715942849, Random Forest Accuracy = 0.7390720045268072, LightGBM Accuracy = 0.7524577410000707\n",
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011521 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 199\n",
            "[LightGBM] [Info] Number of data points in the train set: 56554, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 3: XGBoost Accuracy = 0.7498231715942849, Random Forest Accuracy = 0.7390720045268072, LightGBM Accuracy = 0.75187438110058\n",
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011219 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 56554, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 4: XGBoost Accuracy = 0.7498231715942849, Random Forest Accuracy = 0.7390720045268072, LightGBM Accuracy = 0.7518036497382939\n",
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011213 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 199\n",
            "[LightGBM] [Info] Number of data points in the train set: 56554, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 5: XGBoost Accuracy = 0.7498231715942849, Random Forest Accuracy = 0.7390720045268072, LightGBM Accuracy = 0.7548450983165936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Calculate average accuracy for each model\n",
        "# average_xgb_accuracy = sum(xgb_accuracy_scores) / len(xgb_accuracy_scores)\n",
        "# average_rf_accuracy = sum(rf_accuracy_scores) / len(rf_accuracy_scores)\n",
        "average_lgb_accuracy = sum(lgb_accuracy_scores) / len(lgb_accuracy_scores)\n",
        "\n",
        "# print(f\"Average XGBoost Accuracy: {average_xgb_accuracy}\")\n",
        "# print(f\"Average Random Forest Accuracy: {average_rf_accuracy}\")\n",
        "print(f\"Average LightGBM Accuracy: {average_lgb_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ6PL1FJQcLk",
        "outputId": "c7810843-4fdb-4a0b-c3ca-5c8ad1a32dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average LightGBM Accuracy: 0.7524613978800361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import lightgbm as lgb\n",
        "from sklearn.svm import SVC  # Import SVC\n",
        "\n",
        "# ... (data loading and preprocessing) ...\n",
        "\n",
        "# Create a StratifiedKFold object\n",
        "n_splits = 5  # Number of folds\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store accuracy scores for each fold and each model\n",
        "xgb_accuracy_scores = []\n",
        "rf_accuracy_scores = []\n",
        "lgb_accuracy_scores = []\n",
        "svm_accuracy_scores = []  # List for SVM accuracy scores\n",
        "\n",
        "# Iterate through the folds\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # XGBoost model training and evaluation\n",
        "    # ... (XGBoost code as before) ...\n",
        "\n",
        "    # Random Forest model training and evaluation\n",
        "    # ... (Random Forest code as before) ...\n",
        "\n",
        "    # LightGBM model training and evaluation\n",
        "    # ... (LightGBM code as before) ...\n",
        "\n",
        "    # SVM model training and evaluation\n",
        "    svm_model = SVC(kernel='linear', random_state=42)  # Adjust kernel and other parameters as needed\n",
        "    svm_model.fit(X_train, y_train)\n",
        "    svm_y_pred = svm_model.predict(X_test)\n",
        "    svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
        "    svm_accuracy_scores.append(svm_accuracy)  # Append SVM accuracy to the list\n",
        "\n",
        "    print(f\"Fold {fold + 1}: XGBoost Accuracy = {xgb_accuracy}, Random Forest Accuracy = {rf_accuracy}, LightGBM Accuracy = {lgb_accuracy}, SVM Accuracy = {svm_accuracy}\")\n",
        "\n",
        "# Calculate average accuracy for each model\n",
        "# ... (average accuracy calculations for XGBoost, Random Forest, LightGBM as before) ...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoAhB6k0Q-V1",
        "outputId": "79dfb0c3-d1e5-4e28-eb58-157c35dbfbba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: XGBoost Accuracy = 0.7498231715942849, Random Forest Accuracy = 0.7390720045268072, LightGBM Accuracy = 0.7548450983165936, SVM Accuracy = 0.7515382983237853\n",
            "Fold 2: XGBoost Accuracy = 0.7498231715942849, Random Forest Accuracy = 0.7390720045268072, LightGBM Accuracy = 0.7548450983165936, SVM Accuracy = 0.7479312539783577\n",
            "Fold 3: XGBoost Accuracy = 0.7498231715942849, Random Forest Accuracy = 0.7390720045268072, LightGBM Accuracy = 0.7548450983165936, SVM Accuracy = 0.7469939171028434\n",
            "Fold 4: XGBoost Accuracy = 0.7498231715942849, Random Forest Accuracy = 0.7390720045268072, LightGBM Accuracy = 0.7548450983165936, SVM Accuracy = 0.7449427075965483\n",
            "Fold 5: XGBoost Accuracy = 0.7498231715942849, Random Forest Accuracy = 0.7390720045268072, LightGBM Accuracy = 0.7548450983165936, SVM Accuracy = 0.749893902956571\n",
            "Average SVM Accuracy: 0.7482600159916212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_svm_accuracy = sum(svm_accuracy_scores) / len(svm_accuracy_scores)  # Calculate average SVM accuracy\n",
        "print(f\"Average SVM Accuracy: {average_svm_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCI99fF8gNI8",
        "outputId": "6d24643c-4976-4e9c-b672-da891f819799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average SVM Accuracy: 0.7482600159916212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the meta-model (Logistic Regression in this case)\n",
        "meta_model = LogisticRegression()\n",
        "\n",
        "# Create StratifiedKFold\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store accuracy scores and predictions\n",
        "xgb_accuracy_scores = []\n",
        "rf_accuracy_scores = []\n",
        "lgb_accuracy_scores = []\n",
        "svm_accuracy_scores = []\n",
        "meta_accuracy_scores = []\n",
        "\n",
        "# Meta-model training data\n",
        "meta_train = np.zeros((len(X), 4))  # 4 because we have 4 models\n",
        "meta_test = np.zeros((len(X_test), 4))\n",
        "\n",
        "# Iterate through the folds\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # SVM model\n",
        "    svm_model = SVC(probability=True)\n",
        "    svm_model.fit(X_train, y_train)\n",
        "    svm_y_pred = svm_model.predict(X_test)\n",
        "    svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
        "    svm_accuracy_scores.append(svm_accuracy)\n",
        "\n",
        "    # XGBoost model\n",
        "    xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    xgb_y_pred = xgb_model.predict(X_test)\n",
        "    xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
        "    xgb_accuracy_scores.append(xgb_accuracy)\n",
        "\n",
        "    # Random Forest model\n",
        "    rf_model = RandomForestClassifier(n_estimators=100)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_y_pred = rf_model.predict(X_test)\n",
        "    rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
        "    rf_accuracy_scores.append(rf_accuracy)\n",
        "\n",
        "    # LightGBM model\n",
        "    lgb_train = lgb.Dataset(X_train, label=y_train)\n",
        "    lgb_model = lgb.train(\n",
        "        {'objective': 'binary', 'metric': 'binary_logloss'},\n",
        "        lgb_train,\n",
        "        num_boost_round=100\n",
        "    )\n",
        "    lgb_y_pred = (lgb_model.predict(X_test) > 0.5).astype(int)\n",
        "    lgb_accuracy = accuracy_score(y_test, lgb_y_pred)\n",
        "    lgb_accuracy_scores.append(lgb_accuracy)\n",
        "\n",
        "    # Collect predictions for meta-model\n",
        "    meta_train[test_index, 0] = svm_model.predict_proba(X_test)[:, 1]  # SVM probabilities\n",
        "    meta_train[test_index, 1] = xgb_model.predict_proba(X_test)[:, 1]  # XGBoost probabilities\n",
        "    meta_train[test_index, 2] = rf_model.predict_proba(X_test)[:, 1]   # Random Forest probabilities\n",
        "    meta_train[test_index, 3] = lgb_model.predict(X_test)              # LightGBM probabilities\n",
        "\n",
        "# Train meta-model (Logistic Regression) on the stacked predictions\n",
        "meta_model.fit(meta_train, y)\n",
        "\n",
        "# Meta-model evaluation on test data\n",
        "meta_test[:, 0] = svm_model.predict_proba(X_test)[:, 1]\n",
        "meta_test[:, 1] = xgb_model.predict_proba(X_test)[:, 1]\n",
        "meta_test[:, 2] = rf_model.predict_proba(X_test)[:, 1]\n",
        "meta_test[:, 3] = lgb_model.predict(X_test)\n",
        "\n",
        "meta_y_pred = meta_model.predict(meta_test)\n",
        "meta_accuracy = accuracy_score(y_test, meta_y_pred)\n",
        "meta_accuracy_scores.append(meta_accuracy)\n",
        "\n",
        "print(f\"Meta-model accuracy: {meta_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAmxi1weg9sn",
        "outputId": "3d15d852-2d18-44ad-fc30-8b9b0554a943"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28276\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015590 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 56553, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500009 -> initscore=0.000035\n",
            "[LightGBM] [Info] Start training from score 0.000035\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 28276, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012109 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 197\n",
            "[LightGBM] [Info] Number of data points in the train set: 56553, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000035\n",
            "[LightGBM] [Info] Start training from score -0.000035\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011850 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 199\n",
            "[LightGBM] [Info] Number of data points in the train set: 56554, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012275 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 56554, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "\n",
        "# Ensure X and y are defined\n",
        "# X, y = ...\n",
        "\n",
        "# Initialize the meta-model (Logistic Regression in this case)\n",
        "meta_model = LogisticRegression()\n",
        "\n",
        "# Create StratifiedKFold\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store accuracy scores\n",
        "xgb_accuracy_scores = []\n",
        "rf_accuracy_scores = []\n",
        "lgb_accuracy_scores = []\n",
        "svm_accuracy_scores = []\n",
        "meta_accuracy_scores = []\n",
        "\n",
        "# Meta-model training data\n",
        "meta_train = np.zeros((len(X), 4))  # 4 because we have 4 models\n",
        "\n",
        "# Iterate through the folds\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # SVM model\n",
        "    svm_model = SVC(probability=True)\n",
        "    svm_model.fit(X_train, y_train)\n",
        "    svm_y_pred = svm_model.predict(X_test)\n",
        "    svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
        "    svm_accuracy_scores.append(svm_accuracy)\n",
        "\n",
        "    # XGBoost model\n",
        "    xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    xgb_y_pred = xgb_model.predict(X_test)\n",
        "    xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
        "    xgb_accuracy_scores.append(xgb_accuracy)\n",
        "\n",
        "    # Random Forest model\n",
        "    rf_model = RandomForestClassifier(n_estimators=100)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_y_pred = rf_model.predict(X_test)\n",
        "    rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
        "    rf_accuracy_scores.append(rf_accuracy)\n",
        "\n",
        "    # LightGBM model\n",
        "    lgb_model = lgb.LGBMClassifier(objective='binary', metric='binary_logloss')\n",
        "    lgb_model.fit(X_train, y_train)\n",
        "    lgb_y_pred = lgb_model.predict(X_test)\n",
        "    lgb_accuracy = accuracy_score(y_test, lgb_y_pred)\n",
        "    lgb_accuracy_scores.append(lgb_accuracy)\n",
        "\n",
        "    # Collect predictions for meta-model\n",
        "    meta_train[test_index, 0] = svm_model.predict_proba(X_test)[:, 1]  # SVM probabilities\n",
        "    meta_train[test_index, 1] = xgb_model.predict_proba(X_test)[:, 1]  # XGBoost probabilities\n",
        "    meta_train[test_index, 2] = rf_model.predict_proba(X_test)[:, 1]   # Random Forest probabilities\n",
        "    meta_train[test_index, 3] = lgb_model.predict_proba(X_test)[:, 1]  # LightGBM probabilities\n",
        "\n",
        "# Train meta-model (Logistic Regression) on the stacked predictions\n",
        "meta_model.fit(meta_train, y)\n",
        "\n",
        "# Now create predictions for the test data\n",
        "meta_test = np.zeros((len(X_test), 4))\n",
        "meta_test[:, 0] = svm_model.predict_proba(X_test)[:, 1]\n",
        "meta_test[:, 1] = xgb_model.predict_proba(X_test)[:, 1]\n",
        "meta_test[:, 2] = rf_model.predict_proba(X_test)[:, 1]\n",
        "meta_test[:, 3] = lgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Meta-model evaluation on test data\n",
        "meta_y_pred = meta_model.predict(meta_test)\n",
        "meta_accuracy = accuracy_score(y_test, meta_y_pred)\n",
        "meta_accuracy_scores.append(meta_accuracy)\n",
        "\n",
        "# Print average accuracies for all models\n",
        "print(f\"SVM accuracy: {np.mean(svm_accuracy_scores)}\")\n",
        "print(f\"XGBoost accuracy: {np.mean(xgb_accuracy_scores)}\")\n",
        "print(f\"Random Forest accuracy: {np.mean(rf_accuracy_scores)}\")\n",
        "print(f\"LightGBM accuracy: {np.mean(lgb_accuracy_scores)}\")\n",
        "print(f\"Meta-model accuracy: {meta_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRaUuyFYq4Ln",
        "outputId": "5111a1a4-5ef6-4729-81b0-e379e2a1e7b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28276\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012248 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 56553, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500009 -> initscore=0.000035\n",
            "[LightGBM] [Info] Start training from score 0.000035\n",
            "Fold 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 28276, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012437 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 197\n",
            "[LightGBM] [Info] Number of data points in the train set: 56553, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000035\n",
            "[LightGBM] [Info] Start training from score -0.000035\n",
            "Fold 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011369 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 199\n",
            "[LightGBM] [Info] Number of data points in the train set: 56554, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015756 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 56554, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011222 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 199\n",
            "[LightGBM] [Info] Number of data points in the train set: 56554, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "SVM accuracy: 0.7487551695451115\n",
            "XGBoost accuracy: 0.7482458707196784\n",
            "Random Forest accuracy: 0.7362077660504489\n",
            "LightGBM accuracy: 0.7524613978800361\n",
            "Meta-model accuracy: 0.7549865610411657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure X and y are defined\n",
        "# X, y = ...\n",
        "df = pd.read_csv('/content/drive/MyDrive/Data/dataset_diabetes.csv')\n",
        "\n",
        "y= df['Diabetes_binary']\n",
        "\n",
        "X=df.drop('Diabetes_binary',axis=1)\n",
        "\n",
        "# Initialize the meta-model (Logistic Regression in this case)\n",
        "meta_model = LogisticRegression()\n",
        "\n",
        "# Create StratifiedKFold\n",
        "n_splits = 2\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store accuracy scores\n",
        "xgb_accuracy_scores = []\n",
        "rf_accuracy_scores = []\n",
        "lgb_accuracy_scores = []\n",
        "svm_accuracy_scores = []\n",
        "meta_accuracy_scores = []\n",
        "\n",
        "# Meta-model training data\n",
        "meta_train = np.zeros((len(X), 4))  # 4 because we have 4 models\n",
        "meta_test = np.zeros((len(X), 4))\n",
        "\n",
        "\n",
        "# Iterate through the folds\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # SVM model\n",
        "    # svm_model = SVC(probability=True)\n",
        "    # svm_model.fit(X_train, y_train)\n",
        "    # svm_y_pred = svm_model.predict(X_test)\n",
        "    # svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
        "    # svm_accuracy_scores.append(svm_accuracy)\n",
        "\n",
        "    # svm_model = SVC(kernel='linear', random_state=42, probability=True)  # Adjust kernel and other parameters as needed\n",
        "    # svm_model.fit(X_train, y_train)\n",
        "    # svm_y_pred = svm_model.predict(X_test)\n",
        "    # svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
        "    # svm_accuracy_scores.append(svm_accuracy)\n",
        "\n",
        "    # XGBoost model\n",
        "    # xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "    # xgb_model.fit(X_train, y_train)\n",
        "    # xgb_y_pred = xgb_model.predict(X_test)\n",
        "    # xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
        "    # xgb_accuracy_scores.append(xgb_accuracy)\n",
        "\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    learning_rate=0.1,  # equivalent to 'eta'\n",
        "    max_depth=3,\n",
        "    n_estimators=100  # equivalent to 'num_boost_round'\n",
        "    )\n",
        "\n",
        "    # Fit the model directly with X_train and y_train\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict probabilities and convert them to class predictions\n",
        "    xgb_y_pred = xgb_model.predict(X_test)  # This gives binary predictions\n",
        "    xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
        "\n",
        "    # Store accuracy score\n",
        "    xgb_accuracy_scores.append(xgb_accuracy)\n",
        "\n",
        "    # dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    # dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "    # params = {\n",
        "    #   'objective': 'binary:logistic',\n",
        "    #   'eval_metric': 'logloss',\n",
        "    #   'eta': 0.1,\n",
        "    #   'max_depth': 3\n",
        "    # }\n",
        "    # xgb_model = xgb.train(params, dtrain, num_boost_round=100)\n",
        "    # xgb_y_pred = xgb_model.predict(dtest)\n",
        "    # xgb_accuracy = accuracy_score(y_test, xgb_y_pred.round())\n",
        "    # xgb_accuracy_scores.append(xgb_accuracy)\n",
        "\n",
        "\n",
        "    # Random Forest model\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_y_pred = rf_model.predict(X_test)\n",
        "    rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
        "    rf_accuracy_scores.append(rf_accuracy)\n",
        "\n",
        "    # rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  # Adjust parameters as needed\n",
        "    # rf_model.fit(X_train, y_train)\n",
        "    # rf_y_pred = rf_model.predict(X_test)\n",
        "    # rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
        "    # rf_accuracy_scores.append(rf_accuracy)\n",
        "\n",
        "    # LightGBM model\n",
        "    # lgb_model = lgb.LGBMClassifier(objective='binary', metric='binary_logloss')\n",
        "    # lgb_model.fit(X_train, y_train)\n",
        "    # lgb_y_pred = lgb_model.predict(X_test)\n",
        "    # lgb_accuracy = accuracy_score(y_test, lgb_y_pred)\n",
        "    # lgb_accuracy_scores.append(lgb_accuracy)\n",
        "\n",
        "    lgb_model = lgb.LGBMClassifier(\n",
        "                  objective='binary',  # Same as 'binary'\n",
        "                  metric='binary_logloss',  # Automatically evaluated in fit\n",
        "                  n_estimators=100,  # Equivalent to 'num_boost_round'\n",
        "                  # Add any additional parameters here...\n",
        "                  )\n",
        "\n",
        "    # Fit the model directly with training data\n",
        "    lgb_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_train, y_train), (X_test, y_test)],  # Validation sets\n",
        "        eval_metric='binary_logloss',  # Monitors this metric\n",
        "        # early_stopping_rounds=10,  # Optional: Stops if performance stops improving\n",
        "        # verbose=False  # Set True for detailed logs\n",
        "    )\n",
        "\n",
        "    # Predict probabilities and convert to binary predictions\n",
        "    lgb_y_pred = lgb_model.predict(X_test)  # Direct class prediction\n",
        "    lgb_accuracy = accuracy_score(y_test, lgb_y_pred)\n",
        "\n",
        "    # Store accuracy score\n",
        "    lgb_accuracy_scores.append(lgb_accuracy)\n",
        "\n",
        "    # lgb_train = lgb.Dataset(X_train, label=y_train)\n",
        "    # lgb_test = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
        "    # params = {\n",
        "    #     'objective': 'binary',  # Adjust objective as needed\n",
        "    #     'metric': 'binary_logloss',  # Adjust metric as needed\n",
        "    #     # ... (other LightGBM parameters) ...\n",
        "    # }\n",
        "    # lgb_model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_test], num_boost_round=100)\n",
        "    # lgb_y_pred = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n",
        "    # lgb_accuracy = accuracy_score(y_test, (lgb_y_pred > 0.5).astype(int))  # Assuming binary classification\n",
        "    # lgb_accuracy_scores.append(lgb_accuracy)\n",
        "\n",
        "    # Collect predictions for meta-model\n",
        "    # meta_train[test_index, 0] = svm_model.predict_proba(X_test)[:, 1]  # SVM probabilities\n",
        "    meta_train[test_index, 0] = xgb_model.predict_proba(X_test)[:, 1]  # XGBoost probabilities\n",
        "    meta_train[test_index, 1] = xgb_model.predict_proba(X_test)[:, 1]  # XGBoost probabilities\n",
        "    meta_train[test_index, 2] = rf_model.predict_proba(X_test)[:, 1]   # Random Forest probabilities\n",
        "    meta_train[test_index, 3] = lgb_model.predict_proba(X_test)[:, 1]  # LightGBM probabilities\n",
        "\n",
        "    # meta_test[test_index:, 0] = svm_model.predict_proba(X_test)[:, 1]\n",
        "    # meta_test[test_index:, 1] = xgb_model.predict_proba(X_test)[:, 1]\n",
        "    # meta_test[test_index:, 2] = rf_model.predict_proba(X_test)[:, 1]\n",
        "    # meta_test[test_index:, 3] = lgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Create meta features for the test set\n",
        "    meta_model.fit(meta_train, y_train)\n",
        "\n",
        "    meta_test = np.zeros((len(X_test), 4))\n",
        "    meta_test[:, 0] = xgb_model.predict_proba(X_test)[:, 1]\n",
        "    meta_test[:, 1] = xgb_model.predict_proba(X_test)[:, 1]\n",
        "    meta_test[:, 2] = rf_model.predict_proba(X_test)[:, 1]\n",
        "    meta_test[:, 3] = lgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate meta-model on test set\n",
        "    meta_y_pred = meta_model.predict(meta_test)\n",
        "    meta_accuracy = accuracy_score(y_test, meta_y_pred)\n",
        "    meta_accuracy_scores.append(meta_accuracy)\n",
        "\n",
        "\n",
        "# Train meta-model (Logistic Regression) on the stacked predictions\n",
        "# meta_model.fit(meta_train, y)\n",
        "\n",
        "# # Now create predictions for the test data\n",
        "# meta_test = np.zeros((len(X_test), 4))\n",
        "# # meta_test[:, 0] = svm_model.predict_proba(X_test)[:, 1]\n",
        "# meta_test[:, 1] = xgb_model.predict_proba(X_test)[:, 1]\n",
        "# meta_test[:, 2] = rf_model.predict_proba(X_test)[:, 1]\n",
        "# meta_test[:, 3] = lgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# # Meta-model evaluation on test data\n",
        "# meta_y_pred = meta_model.predict(meta_test)\n",
        "# meta_accuracy = accuracy_score(y_test, meta_y_pred)\n",
        "# meta_accuracy_scores.append(meta_accuracy)\n",
        "\n",
        "# Print average accuracies for all models\n",
        "print(f\"SVM accuracy: {np.mean(svm_accuracy_scores)}\")\n",
        "print(f\"XGBoost accuracy: {np.mean(xgb_accuracy_scores)}\")\n",
        "print(f\"Random Forest accuracy: {np.mean(rf_accuracy_scores)}\")\n",
        "print(f\"LightGBM accuracy: {np.mean(lgb_accuracy_scores)}\")\n",
        "print(f\"Meta-model accuracy: {np.mean(meta_accuracy_scores)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "XPb4CMYD-m9t",
        "outputId": "4ae99e00-8540-4f74-e711-000d2e3da4a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n",
            "[LightGBM] [Info] Number of positive: 17673, number of negative: 17673\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006918 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 192\n",
            "[LightGBM] [Info] Number of data points in the train set: 35346, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-160908eff32c>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;31m# Collect predictions for meta-model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;31m# meta_train[test_index, 0] = svm_model.predict_proba(X_test)[:, 1]  # SVM probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mmeta_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# XGBoost probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mmeta_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# XGBoost probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mmeta_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Random Forest probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Data/dataset_diabetes.csv')\n",
        "y = df['Diabetes_binary']\n",
        "X = df.drop('Diabetes_binary', axis=1)\n",
        "\n",
        "# Initialize the meta-model\n",
        "meta_model = LogisticRegression()\n",
        "\n",
        "# Create StratifiedKFold\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize accuracy score lists\n",
        "xgb_accuracy_scores = []\n",
        "rf_accuracy_scores = []\n",
        "lgb_accuracy_scores = []\n",
        "meta_accuracy_scores = []\n",
        "svm_accuracy_scores=[]\n",
        "\n",
        "# Meta-model training data\n",
        "meta_train = np.zeros((len(X), 4))  # 3 models contribute predictions\n",
        "\n",
        "# Stratified K-Fold Cross Validation\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # XGBoost model\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='logloss',\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        n_estimators=100\n",
        "    )\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    meta_train[test_index, 0] = xgb_model.predict_proba(X_test)[:, 1]\n",
        "    xgb_y_pred = xgb_model.predict(X_test)\n",
        "    xgb_accuracy_scores.append(accuracy_score(y_test, xgb_y_pred))\n",
        "\n",
        "    # Random Forest model\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    meta_train[test_index, 1] = rf_model.predict_proba(X_test)[:, 1]\n",
        "    rf_y_pred = rf_model.predict(X_test)\n",
        "    rf_accuracy_scores.append(accuracy_score(y_test, rf_y_pred))\n",
        "\n",
        "    # LightGBM model\n",
        "    lgb_model = lgb.LGBMClassifier(\n",
        "        objective='binary',\n",
        "        metric='binary_logloss',\n",
        "        n_estimators=100\n",
        "    )\n",
        "    lgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='binary_logloss')\n",
        "    meta_train[test_index, 2] = lgb_model.predict_proba(X_test)[:, 1]\n",
        "    lgb_y_pred = lgb_model.predict(X_test)\n",
        "    lgb_accuracy_scores.append(accuracy_score(y_test, lgb_y_pred))\n",
        "\n",
        "    svm_model = SVC(probability=True)  # Adjust kernel and other parameters as needed\n",
        "    svm_model.fit(X_train, y_train)\n",
        "    meta_train[test_index, 3] = svm_model.predict_proba(X_test)[:, 1]\n",
        "    svm_y_pred = svm_model.predict(X_test)\n",
        "    svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
        "    svm_accuracy_scores.append(svm_accuracy)\n",
        "\n",
        "# Train meta-model (Logistic Regression) on stacked predictions\n",
        "meta_model.fit(meta_train, y)\n",
        "\n",
        "# Evaluate meta-model\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    meta_test = meta_train[test_index]\n",
        "    meta_y_pred = meta_model.predict(meta_test)\n",
        "    meta_accuracy = accuracy_score(y.iloc[test_index], meta_y_pred)\n",
        "    meta_accuracy_scores.append(meta_accuracy)\n",
        "\n",
        "# Print average accuracies for all models\n",
        "print(f\"SVM accuracy: {np.mean(svm_accuracy_scores):.4f}\")\n",
        "print(f\"XGBoost accuracy: {np.mean(xgb_accuracy_scores):.4f}\")\n",
        "print(f\"Random Forest accuracy: {np.mean(rf_accuracy_scores):.4f}\")\n",
        "print(f\"LightGBM accuracy: {np.mean(lgb_accuracy_scores):.4f}\")\n",
        "print(f\"Meta-model accuracy: {np.mean(meta_accuracy_scores):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OagQpEI7aQln",
        "outputId": "45d87476-104f-450e-f172-fc249c14c481"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n",
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28276\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016398 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 56553, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500009 -> initscore=0.000035\n",
            "[LightGBM] [Info] Start training from score 0.000035\n",
            "Fold 2\n",
            "[LightGBM] [Info] Number of positive: 28276, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010821 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 197\n",
            "[LightGBM] [Info] Number of data points in the train set: 56553, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000035\n",
            "[LightGBM] [Info] Start training from score -0.000035\n",
            "Fold 3\n",
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015481 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 199\n",
            "[LightGBM] [Info] Number of data points in the train set: 56554, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 4\n",
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011152 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 56554, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Fold 5\n",
            "[LightGBM] [Info] Number of positive: 28277, number of negative: 28277\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011105 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 199\n",
            "[LightGBM] [Info] Number of data points in the train set: 56554, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "SVM accuracy: 0.7488\n",
            "XGBoost accuracy: 0.7521\n",
            "Random Forest accuracy: 0.7370\n",
            "LightGBM accuracy: 0.7525\n",
            "Meta-model accuracy: 0.7527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"SVM accuracy: {np.mean(svm_accuracy_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "RM3wFCwIr0vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Data/dataset_diabetes.csv')\n",
        "y = df['Diabetes_binary']\n",
        "X = df.drop('Diabetes_binary', axis=1)\n",
        "\n",
        "# Create StratifiedKFold\n",
        "n_splits = 2\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize accuracy score lists\n",
        "xgb_accuracy_scores = []\n",
        "rf_accuracy_scores = []\n",
        "lgb_accuracy_scores = []\n",
        "meta_accuracy_scores = []\n",
        "\n",
        "# Meta-model (Voting Classifier) training data\n",
        "meta_train = np.zeros((len(X), 3))  # 3 models contribute predictions\n",
        "\n",
        "# Stratified K-Fold Cross Validation\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # XGBoost model\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='logloss',\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        n_estimators=100\n",
        "    )\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    meta_train[test_index, 0] = xgb_model.predict(X_test)\n",
        "    xgb_y_pred = xgb_model.predict(X_test)\n",
        "    xgb_accuracy_scores.append(accuracy_score(y_test, xgb_y_pred))\n",
        "\n",
        "    # Random Forest model\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    meta_train[test_index, 1] = rf_model.predict(X_test)\n",
        "    rf_y_pred = rf_model.predict(X_test)\n",
        "    rf_accuracy_scores.append(accuracy_score(y_test, rf_y_pred))\n",
        "\n",
        "    # LightGBM model\n",
        "    lgb_model = lgb.LGBMClassifier(\n",
        "        objective='binary',\n",
        "        metric='binary_logloss',\n",
        "        n_estimators=100\n",
        "    )\n",
        "    lgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='binary_logloss', verbose=False)\n",
        "    meta_train[test_index, 2] = lgb_model.predict(X_test)\n",
        "    lgb_y_pred = lgb_model.predict(X_test)\n",
        "    lgb_accuracy_scores.append(accuracy_score(y_test, lgb_y_pred))\n",
        "\n",
        "# Create VotingClassifier for majority voting\n",
        "voting_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', learning_rate=0.1, max_depth=3, n_estimators=100)),\n",
        "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "        ('lgb', lgb.LGBMClassifier(objective='binary', metric='binary_logloss', n_estimators=100))\n",
        "    ],\n",
        "    voting='hard'  # 'hard' for majority voting\n",
        ")\n",
        "\n",
        "# Fit VotingClassifier (majority voting)\n",
        "voting_model.fit(meta_train, y)\n",
        "\n",
        "# Evaluate meta-model (VotingClassifier)\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    meta_test = meta_train[test_index]\n",
        "    meta_y_pred = voting_model.predict(meta_test)\n",
        "    meta_accuracy = accuracy_score(y.iloc[test_index], meta_y_pred)\n",
        "    meta_accuracy_scores.append(meta_accuracy)\n",
        "\n",
        "# Print average accuracies for all models\n",
        "print(f\"XGBoost accuracy: {np.mean(xgb_accuracy_scores):.4f}\")\n",
        "print(f\"Random Forest accuracy: {np.mean(rf_accuracy_scores):.4f}\")\n",
        "print(f\"LightGBM accuracy: {np.mean(lgb_accuracy_scores):.4f}\")\n",
        "print(f\"Meta-model (Majority Voting) accuracy: {np.mean(meta_accuracy_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "7vGOub1drqwW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}